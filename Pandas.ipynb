{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Tutorial: From Basic to Advanced\n",
    "\n",
    "# First, you need to install pandas if you haven't already.\n",
    "# You can do this by running: pip install pandas\n",
    "# If you're using Anaconda, pandas usually comes pre-installed.\n",
    "\n",
    "# Import the pandas library, commonly aliased as 'pd'\n",
    "import pandas as pd\n",
    "# Import numpy, which pandas often relies on, aliased as 'np'\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pandas Series ---\n",
    "# A Series is a one-dimensional labeled array capable of holding any data type.\n",
    "\n",
    "print(\"\\n--- 1. Pandas Series ---\")\n",
    "# Creating a Series from a list\n",
    "data_list = [10, 20, 30, 40, 50]\n",
    "s1 = pd.Series(data_list) # Creates a Series with default integer index (0, 1, 2, ...)\n",
    "print(\"Series from list (s1):\\n\", s1)\n",
    "\n",
    "# Accessing elements in a Series (similar to lists/arrays)\n",
    "print(\"\\nFirst element of s1:\", s1[0]) # Access the element at index 0\n",
    "print(\"Elements from index 1 to 3 (exclusive) of s1:\\n\", s1[1:3]) # Slicing\n",
    "\n",
    "# Creating a Series with a custom index\n",
    "custom_index = ['a', 'b', 'c', 'd', 'e']\n",
    "s2 = pd.Series(data_list, index=custom_index) # Creates a Series with specified labels as index\n",
    "print(\"\\nSeries with custom index (s2):\\n\", s2)\n",
    "\n",
    "# Accessing elements using custom index\n",
    "print(\"\\nElement at index 'b' of s2:\", s2['b']) # Access element using its label\n",
    "print(\"Value of s2:\", s2.values) # Get the underlying numpy array of values\n",
    "print(\"Index of s2:\", s2.index) # Get the index object\n",
    "\n",
    "# Creating a Series from a dictionary\n",
    "data_dict_series = {'apple': 5, 'banana': 8, 'cherry': 3}\n",
    "s3 = pd.Series(data_dict_series) # Dictionary keys become the index, values become the Series data\n",
    "print(\"\\nSeries from dictionary (s3):\\n\", s3)\n",
    "\n",
    "# Basic operations on Series\n",
    "print(\"\\nAdding 5 to each element in s1:\\n\", s1 + 5) # Element-wise addition\n",
    "print(\"Elements in s1 greater than 25:\\n\", s1[s1 > 25]) # Boolean indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Pandas DataFrame ---\n",
    "# A DataFrame is a two-dimensional, size-mutable, and potentially heterogeneous\n",
    "# tabular data structure with labeled axes (rows and columns).\n",
    "\n",
    "print(\"\\n\\n--- 2. Pandas DataFrame ---\")\n",
    "# Creating a DataFrame from a dictionary of lists\n",
    "data_dict_df = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'Paris', 'London', 'Berlin']\n",
    "}\n",
    "df1 = pd.DataFrame(data_dict_df) # Keys become column names, lists become column data\n",
    "print(\"DataFrame from dictionary of lists (df1):\\n\", df1)\n",
    "\n",
    "# Creating a DataFrame with a custom index\n",
    "custom_row_index = ['id1', 'id2', 'id3', 'id4']\n",
    "df2 = pd.DataFrame(data_dict_df, index=custom_row_index) # Assigns custom labels to rows\n",
    "print(\"\\nDataFrame with custom row index (df2):\\n\", df2)\n",
    "\n",
    "# Creating a DataFrame from a list of dictionaries\n",
    "data_list_of_dicts = [\n",
    "    {'Name': 'Eve', 'Age': 22, 'City': 'Tokyo'},\n",
    "    {'Name': 'Frank', 'Age': 29, 'City': 'Sydney'}\n",
    "]\n",
    "df3 = pd.DataFrame(data_list_of_dicts) # Each dictionary becomes a row\n",
    "print(\"\\nDataFrame from list of dictionaries (df3):\\n\", df3)\n",
    "\n",
    "# Creating a DataFrame from a NumPy array\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "df_numpy = pd.DataFrame(np_array, columns=['ColA', 'ColB', 'ColC']) # Creates DataFrame with specified column names\n",
    "print(\"\\nDataFrame from NumPy array (df_numpy):\\n\", df_numpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Data Loading and Saving ---\n",
    "# Pandas can easily read and write data from/to various file formats.\n",
    "\n",
    "print(\"\\n\\n--- 3. Data Loading and Saving ---\")\n",
    "# For this section, let's create a sample CSV file to read from.\n",
    "sample_data_for_csv = {\n",
    "    'col1': [1, 2, 3, 4],\n",
    "    'col2': ['A', 'B', 'C', 'D'],\n",
    "    'col3': [1.1, 2.2, 3.3, 4.4]\n",
    "}\n",
    "df_to_save = pd.DataFrame(sample_data_for_csv)\n",
    "# Save DataFrame to a CSV file\n",
    "df_to_save.to_csv('sample_data.csv', index=False) # index=False prevents writing row index to CSV\n",
    "print(\"Saved df_to_save to 'sample_data.csv'\")\n",
    "\n",
    "# Read data from a CSV file\n",
    "df_from_csv = pd.read_csv('sample_data.csv') # Reads the CSV file into a DataFrame\n",
    "print(\"\\nDataFrame read from 'sample_data.csv':\\n\", df_from_csv)\n",
    "\n",
    "# You can also read/write other formats like Excel, JSON, SQL, etc.\n",
    "# Example: df.to_excel('output.xlsx', sheet_name='Sheet1')\n",
    "# Example: pd.read_excel('input.xlsx')\n",
    "# Example: df.to_json('output.json', orient='records', lines=True)\n",
    "# Example: pd.read_json('input.json', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Data Inspection and Exploration ---\n",
    "# Understanding your data is crucial.\n",
    "\n",
    "print(\"\\n\\n--- 4. Data Inspection and Exploration ---\")\n",
    "# Let's use df1 for inspection\n",
    "print(\"Original DataFrame (df1):\\n\", df1)\n",
    "\n",
    "# Get the first N rows (default is 5)\n",
    "print(\"\\nFirst 2 rows of df1 (head):\\n\", df1.head(2))\n",
    "\n",
    "# Get the last N rows (default is 5)\n",
    "print(\"\\nLast 2 rows of df1 (tail):\\n\", df1.tail(2))\n",
    "\n",
    "# Get a concise summary of the DataFrame\n",
    "print(\"\\nInfo about df1:\")\n",
    "df1.info() # Provides data types, non-null counts, memory usage\n",
    "\n",
    "# Get descriptive statistics\n",
    "print(\"\\nDescriptive statistics of df1 (describe):\\n\", df1.describe()) # For numerical columns\n",
    "print(\"\\nDescriptive statistics for all columns (include='all'):\\n\", df1.describe(include='all')) # Includes non-numerical\n",
    "\n",
    "# Get the dimensions of the DataFrame (rows, columns)\n",
    "print(\"\\nShape of df1 (rows, columns):\", df1.shape)\n",
    "\n",
    "# Get the data types of each column\n",
    "print(\"\\nData types of columns in df1:\\n\", df1.dtypes)\n",
    "\n",
    "# Get the column names\n",
    "print(\"\\nColumn names of df1:\", df1.columns)\n",
    "\n",
    "# Get the index (row labels)\n",
    "print(\"\\nIndex of df1:\", df1.index)\n",
    "\n",
    "# Get unique values in a column\n",
    "print(\"\\nUnique cities in df1:\", df1['City'].unique())\n",
    "\n",
    "# Get the count of unique values in a column\n",
    "print(\"\\nNumber of unique cities in df1:\", df1['City'].nunique())\n",
    "\n",
    "# Get the frequency of each unique value in a column\n",
    "print(\"\\nValue counts for 'City' column in df1:\\n\", df1['City'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Data Selection and Filtering ---\n",
    "# Accessing specific parts of your data.\n",
    "\n",
    "print(\"\\n\\n--- 5. Data Selection and Filtering ---\")\n",
    "# Selecting a single column (returns a Series)\n",
    "ages = df1['Age'] # Selects the 'Age' column\n",
    "print(\"Selected 'Age' column (Series):\\n\", ages)\n",
    "\n",
    "# Selecting multiple columns (returns a DataFrame)\n",
    "name_and_city = df1[['Name', 'City']] # Pass a list of column names\n",
    "print(\"\\nSelected 'Name' and 'City' columns (DataFrame):\\n\", name_and_city)\n",
    "\n",
    "# --- Row Selection ---\n",
    "# Using .loc[] (label-based selection)\n",
    "# df1 has default integer index (0, 1, 2, 3)\n",
    "print(\"\\nRow at index 0 using .loc:\\n\", df1.loc[0]) # Selects the first row by its label (which is 0 here)\n",
    "print(\"\\nRows at index 0 and 2 using .loc:\\n\", df1.loc[[0, 2]]) # Selects multiple rows by label\n",
    "\n",
    "# Using .iloc[] (integer position-based selection)\n",
    "print(\"\\nRow at integer position 1 using .iloc:\\n\", df1.iloc[1]) # Selects the second row by its integer position\n",
    "print(\"\\nRows from position 0 to 2 (exclusive) using .iloc:\\n\", df1.iloc[0:2]) # Slicing rows\n",
    "\n",
    "# Selecting specific rows and columns using .loc\n",
    "print(\"\\n'Name' and 'Age' for rows at index 0 and 1 using .loc:\\n\", df1.loc[[0, 1], ['Name', 'Age']])\n",
    "\n",
    "# Selecting specific rows and columns using .iloc\n",
    "print(\"\\nColumns at position 0 and 2 for rows at position 1 and 2 using .iloc:\\n\", df1.iloc[[1, 2], [0, 2]])\n",
    "\n",
    "# --- Boolean Indexing (Filtering) ---\n",
    "# Select rows based on a condition\n",
    "young_people = df1[df1['Age'] < 30] # Selects rows where 'Age' is less than 30\n",
    "print(\"\\nPeople younger than 30:\\n\", young_people)\n",
    "\n",
    "# Multiple conditions (use & for AND, | for OR, and wrap conditions in parentheses)\n",
    "young_new_yorkers = df1[(df1['Age'] < 30) & (df1['City'] == 'New York')]\n",
    "print(\"\\nPeople younger than 30 AND from New York:\\n\", young_new_yorkers)\n",
    "\n",
    "# Using .isin() for filtering based on a list of values\n",
    "selected_cities = df1[df1['City'].isin(['New York', 'London'])]\n",
    "print(\"\\nPeople from New York or London:\\n\", selected_cities)\n",
    "\n",
    "# Using .query() method for filtering (string-based querying)\n",
    "query_result = df1.query('Age > 28 and City != \"Paris\"') # More readable for complex conditions\n",
    "print(\"\\nQuery result (Age > 28 and City != 'Paris'):\\n\", query_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Data Cleaning ---\n",
    "# Handling missing data, duplicates, etc.\n",
    "\n",
    "print(\"\\n\\n--- 6. Data Cleaning ---\")\n",
    "# Create a DataFrame with missing values (NaN - Not a Number)\n",
    "data_missing = {\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 7, 8, np.nan, 10],\n",
    "    'C': ['x', 'y', 'z', 'x', 'y']\n",
    "}\n",
    "df_missing = pd.DataFrame(data_missing)\n",
    "print(\"DataFrame with missing values (df_missing):\\n\", df_missing)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nAre there missing values? (isnull):\\n\", df_missing.isnull())\n",
    "print(\"\\nSum of missing values per column:\\n\", df_missing.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "# Fill with a specific value\n",
    "df_filled_value = df_missing.fillna(value=0) # Replaces all NaN with 0\n",
    "print(\"\\nDataFrame with NaNs filled with 0:\\n\", df_filled_value)\n",
    "\n",
    "# Fill with mean of the column (only for numerical columns)\n",
    "df_missing['A_filled_mean'] = df_missing['A'].fillna(df_missing['A'].mean()) # Fills NaN in 'A' with mean of 'A'\n",
    "print(\"\\nDataFrame with 'A' NaNs filled with mean:\\n\", df_missing)\n",
    "\n",
    "# Forward fill (propagate last valid observation forward)\n",
    "df_ffill = df_missing.fillna(method='ffill') # Fills NaN with the value from the previous row\n",
    "print(\"\\nDataFrame with NaNs forward-filled:\\n\", df_ffill)\n",
    "\n",
    "# Backward fill (propagate next valid observation backward)\n",
    "df_bfill = df_missing.fillna(method='bfill') # Fills NaN with the value from the next row\n",
    "print(\"\\nDataFrame with NaNs backward-filled:\\n\", df_bfill)\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_dropped_rows = df_missing.dropna() # Removes rows containing at least one NaN\n",
    "print(\"\\nDataFrame with rows containing NaNs dropped:\\n\", df_dropped_rows)\n",
    "\n",
    "# Drop columns with any missing values\n",
    "df_dropped_cols = df_missing.dropna(axis=1) # axis=1 specifies columns\n",
    "print(\"\\nDataFrame with columns containing NaNs dropped:\\n\", df_dropped_cols)\n",
    "\n",
    "# Handling duplicates\n",
    "data_duplicates = {\n",
    "    'ID': [1, 2, 2, 3, 4, 4, 4],\n",
    "    'Name': ['A', 'B', 'B', 'C', 'D', 'D', 'D']\n",
    "}\n",
    "df_duplicates = pd.DataFrame(data_duplicates)\n",
    "print(\"\\nDataFrame with duplicates (df_duplicates):\\n\", df_duplicates)\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nAre rows duplicated?:\\n\", df_duplicates.duplicated())\n",
    "\n",
    "# Drop duplicate rows (keeps the first occurrence by default)\n",
    "df_no_duplicates = df_duplicates.drop_duplicates()\n",
    "print(\"\\nDataFrame with duplicates removed (keeping first):\\n\", df_no_duplicates)\n",
    "\n",
    "# Drop duplicates, keeping the last occurrence\n",
    "df_no_duplicates_last = df_duplicates.drop_duplicates(keep='last')\n",
    "print(\"\\nDataFrame with duplicates removed (keeping last):\\n\", df_no_duplicates_last)\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "df_no_duplicates_subset = df_duplicates.drop_duplicates(subset=['ID'], keep='first')\n",
    "print(\"\\nDataFrame with duplicates removed based on 'ID' column:\\n\", df_no_duplicates_subset)\n",
    "\n",
    "# Changing data types\n",
    "df_types = pd.DataFrame({'A': ['1', '2', '3'], 'B': [1.0, 2.5, 3.7]})\n",
    "print(\"\\nOriginal DataFrame for type change (df_types):\\n\", df_types)\n",
    "print(\"Original dtypes:\\n\", df_types.dtypes)\n",
    "df_types['A'] = df_types['A'].astype(int) # Converts column 'A' from object (string) to integer\n",
    "df_types['B'] = df_types['B'].astype(str) # Converts column 'B' from float to object (string)\n",
    "print(\"\\nDataFrame with changed dtypes:\\n\", df_types)\n",
    "print(\"New dtypes:\\n\", df_types.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 7. Data Transformation ---\n",
    "# Modifying data, applying functions, etc.\n",
    "\n",
    "print(\"\\n\\n--- 7. Data Transformation ---\")\n",
    "# Let's use df1 again\n",
    "print(\"Original DataFrame (df1):\\n\", df1)\n",
    "\n",
    "# Applying a function to a column\n",
    "# Using .apply() with a lambda function\n",
    "df1['Age_in_5_Years'] = df1['Age'].apply(lambda x: x + 5) # Adds 5 to each value in 'Age' column\n",
    "print(\"\\nDataFrame with 'Age_in_5_Years' column (using apply):\\n\", df1)\n",
    "\n",
    "# Using .map() on a Series (often for mapping values based on a dictionary or function)\n",
    "city_map = {'New York': 'USA', 'Paris': 'France', 'London': 'UK', 'Berlin': 'Germany'}\n",
    "df1['Country'] = df1['City'].map(city_map) # Maps city names to country names\n",
    "print(\"\\nDataFrame with 'Country' column (using map):\\n\", df1)\n",
    "\n",
    "# Using .applymap() to apply a function element-wise to the entire DataFrame (or a selection)\n",
    "# (Be cautious with applymap on mixed-type DataFrames)\n",
    "df_numeric_example = pd.DataFrame(np.random.randn(3, 3), columns=['A', 'B', 'C'])\n",
    "print(\"\\nNumeric DataFrame for applymap:\\n\", df_numeric_example)\n",
    "df_abs_values = df_numeric_example.applymap(abs) # Applies abs function to every element\n",
    "print(\"\\nAbsolute values using applymap:\\n\", df_abs_values)\n",
    "\n",
    "# Adding a new column\n",
    "df1['Salary'] = [50000, 60000, 70000, 55000] # Adds a new 'Salary' column with specified values\n",
    "print(\"\\nDataFrame with new 'Salary' column:\\n\", df1)\n",
    "\n",
    "# Removing a column\n",
    "df1_dropped_salary = df1.drop('Salary', axis=1) # axis=1 indicates column, inplace=True modifies original df\n",
    "print(\"\\nDataFrame with 'Salary' column removed (returns new df):\\n\", df1_dropped_salary)\n",
    "# To modify df1 in place: df1.drop('Salary', axis=1, inplace=True)\n",
    "\n",
    "# Renaming columns\n",
    "df1_renamed = df1.rename(columns={'Name': 'Full Name', 'Age': 'Years Old'}) # Renames specified columns\n",
    "print(\"\\nDataFrame with renamed columns:\\n\", df1_renamed)\n",
    "# To modify df1 in place: df1.rename(columns={'Name': 'Full Name'}, inplace=True)\n",
    "\n",
    "# Sorting data\n",
    "# Sort by values in a column\n",
    "df_sorted_age = df1.sort_values(by='Age', ascending=False) # Sorts by 'Age' in descending order\n",
    "print(\"\\nDataFrame sorted by Age (descending):\\n\", df_sorted_age)\n",
    "\n",
    "# Sort by multiple columns\n",
    "df_sorted_city_age = df1.sort_values(by=['City', 'Age'], ascending=[True, False]) # Sorts by City (asc), then Age (desc)\n",
    "print(\"\\nDataFrame sorted by City (asc) then Age (desc):\\n\", df_sorted_city_age)\n",
    "\n",
    "# Sort by index\n",
    "df_sorted_index = df2.sort_index(ascending=False) # Sorts by row index labels in descending order\n",
    "print(\"\\nDataFrame df2 sorted by index (descending):\\n\", df_sorted_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 8. Grouping and Aggregation ---\n",
    "# `groupby()` is one of the most powerful features.\n",
    "\n",
    "print(\"\\n\\n--- 8. Grouping and Aggregation ---\")\n",
    "# Let's create a more complex DataFrame for grouping\n",
    "data_grouping = {\n",
    "    'Department': ['HR', 'IT', 'Sales', 'IT', 'HR', 'Sales', 'IT'],\n",
    "    'Employee': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace'],\n",
    "    'Salary': [70000, 80000, 75000, 90000, 65000, 85000, 95000],\n",
    "    'YearsExperience': [5, 3, 7, 4, 2, 6, 8]\n",
    "}\n",
    "df_group = pd.DataFrame(data_grouping)\n",
    "print(\"DataFrame for grouping (df_group):\\n\", df_group)\n",
    "\n",
    "# Group by 'Department'\n",
    "grouped_by_dept = df_group.groupby('Department') # Creates a DataFrameGroupBy object\n",
    "print(\"\\nGrouped by Department (object):\", grouped_by_dept)\n",
    "\n",
    "# Calculate mean salary for each department\n",
    "mean_salary_dept = grouped_by_dept['Salary'].mean() # Applies mean() aggregation to 'Salary' for each group\n",
    "print(\"\\nMean salary per department:\\n\", mean_salary_dept)\n",
    "\n",
    "# Calculate sum of 'YearsExperience' for each department\n",
    "sum_experience_dept = grouped_by_dept['YearsExperience'].sum() # Applies sum() aggregation\n",
    "print(\"\\nSum of years of experience per department:\\n\", sum_experience_dept)\n",
    "\n",
    "# Count employees in each department\n",
    "count_employees_dept = grouped_by_dept['Employee'].count() # Applies count() aggregation\n",
    "print(\"\\nNumber of employees per department:\\n\", count_employees_dept)\n",
    "\n",
    "# Multiple aggregations on a single column\n",
    "agg_salary_dept = grouped_by_dept['Salary'].agg(['mean', 'min', 'max', 'count']) # Applies multiple aggregations\n",
    "print(\"\\nMultiple aggregations on 'Salary' per department:\\n\", agg_salary_dept)\n",
    "\n",
    "# Multiple aggregations on multiple columns\n",
    "agg_multiple_cols = grouped_by_dept.agg({\n",
    "    'Salary': ['mean', 'sum'],          # Mean and sum for 'Salary'\n",
    "    'YearsExperience': ['min', 'max']   # Min and max for 'YearsExperience'\n",
    "})\n",
    "print(\"\\nMultiple aggregations on multiple columns per department:\\n\", agg_multiple_cols)\n",
    "\n",
    "# Group by multiple columns\n",
    "grouped_by_dept_exp_level = df_group.groupby(['Department', pd.cut(df_group['YearsExperience'], bins=[0, 3, 6, 10])])\n",
    "# pd.cut is used to segment and sort data values into bins.\n",
    "# Here, we bin 'YearsExperience' into (0,3], (3,6], (6,10]\n",
    "mean_salary_dept_exp_level = grouped_by_dept_exp_level['Salary'].mean()\n",
    "print(\"\\nMean salary by Department and Experience Level:\\n\", mean_salary_dept_exp_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 9. Merging, Joining, and Concatenating ---\n",
    "# Combining multiple DataFrames.\n",
    "\n",
    "print(\"\\n\\n--- 9. Merging, Joining, and Concatenating ---\")\n",
    "\n",
    "# --- Concatenating ---\n",
    "# Stacks DataFrames along an axis (rows or columns).\n",
    "df_concat1 = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']})\n",
    "df_concat2 = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']})\n",
    "print(\"df_concat1:\\n\", df_concat1)\n",
    "print(\"df_concat2:\\n\", df_concat2)\n",
    "\n",
    "# Concatenate along rows (axis=0, default)\n",
    "concatenated_rows = pd.concat([df_concat1, df_concat2]) # Appends df_concat2 rows to df_concat1\n",
    "print(\"\\nConcatenated along rows (default):\\n\", concatenated_rows)\n",
    "\n",
    "# Concatenate along rows, resetting index\n",
    "concatenated_rows_reset_index = pd.concat([df_concat1, df_concat2], ignore_index=True) # Creates a new default index\n",
    "print(\"\\nConcatenated along rows (reset index):\\n\", concatenated_rows_reset_index)\n",
    "\n",
    "df_concat3 = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']}, index=[0, 1])\n",
    "# Concatenate along columns (axis=1)\n",
    "concatenated_cols = pd.concat([df_concat1, df_concat3], axis=1) # Joins columns side-by-side based on index\n",
    "print(\"\\nConcatenated along columns (axis=1):\\n\", concatenated_cols)\n",
    "\n",
    "\n",
    "# --- Merging ---\n",
    "# SQL-style joins.\n",
    "df_left = pd.DataFrame({\n",
    "    'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "    'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "    'B': ['B0', 'B1', 'B2', 'B3']\n",
    "})\n",
    "df_right = pd.DataFrame({\n",
    "    'key': ['K0', 'K1', 'K4', 'K5'], # K2, K3 missing; K4, K5 new\n",
    "    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "    'D': ['D0', 'D1', 'D2', 'D3']\n",
    "})\n",
    "print(\"\\ndf_left:\\n\", df_left)\n",
    "print(\"df_right:\\n\", df_right)\n",
    "\n",
    "# Inner merge (default): only keys present in both DataFrames\n",
    "inner_merged = pd.merge(df_left, df_right, on='key', how='inner') # Joins on 'key' column\n",
    "print(\"\\nInner merge on 'key':\\n\", inner_merged) # K0, K1\n",
    "\n",
    "# Left merge: all keys from left DataFrame, matching keys from right\n",
    "left_merged = pd.merge(df_left, df_right, on='key', how='left') # Keeps all keys from df_left\n",
    "print(\"\\nLeft merge on 'key':\\n\", left_merged) # K0, K1, K2, K3 (C, D will be NaN for K2, K3)\n",
    "\n",
    "# Right merge: all keys from right DataFrame, matching keys from left\n",
    "right_merged = pd.merge(df_left, df_right, on='key', how='right') # Keeps all keys from df_right\n",
    "print(\"\\nRight merge on 'key':\\n\", right_merged) # K0, K1, K4, K5 (A, B will be NaN for K4, K5)\n",
    "\n",
    "# Outer merge: all keys from both DataFrames\n",
    "outer_merged = pd.merge(df_left, df_right, on='key', how='outer') # Keeps all keys from both\n",
    "print(\"\\nOuter merge on 'key':\\n\", outer_merged) # K0, K1, K2, K3, K4, K5\n",
    "\n",
    "# Merging on multiple keys\n",
    "df_left_multi = pd.DataFrame({\n",
    "    'key1': ['K0', 'K0', 'K1', 'K2'],\n",
    "    'key2': ['X0', 'X1', 'X0', 'X1'],\n",
    "    'A': ['A0', 'A1', 'A2', 'A3']\n",
    "})\n",
    "df_right_multi = pd.DataFrame({\n",
    "    'key1': ['K0', 'K1', 'K1', 'K2'],\n",
    "    'key2': ['X0', 'X0', 'X0', 'X0'], # Note: X1 in df_left_multi vs X0 here\n",
    "    'B': ['B0', 'B1', 'B2', 'B3']\n",
    "})\n",
    "print(\"\\ndf_left_multi:\\n\", df_left_multi)\n",
    "print(\"df_right_multi:\\n\", df_right_multi)\n",
    "merged_multi_keys = pd.merge(df_left_multi, df_right_multi, on=['key1', 'key2'], how='inner')\n",
    "print(\"\\nInner merge on multiple keys ('key1', 'key2'):\\n\", merged_multi_keys)\n",
    "\n",
    "# --- Joining ---\n",
    "# Similar to merge, but primarily joins on index by default.\n",
    "df_join_left = pd.DataFrame({'A': ['A0', 'A1', 'A2']}, index=['idx0', 'idx1', 'idx2'])\n",
    "df_join_right = pd.DataFrame({'B': ['B0', 'B2', 'B3']}, index=['idx0', 'idx2', 'idx3'])\n",
    "print(\"\\ndf_join_left:\\n\", df_join_left)\n",
    "print(\"df_join_right:\\n\", df_join_right)\n",
    "\n",
    "# Default join is left join on index\n",
    "joined_df = df_join_left.join(df_join_right) # Performs a left join using indices\n",
    "print(\"\\nJoined DataFrame (left join on index by default):\\n\", joined_df)\n",
    "\n",
    "# Outer join on index\n",
    "joined_outer_df = df_join_left.join(df_join_right, how='outer') # Performs an outer join using indices\n",
    "print(\"\\nOuter joined DataFrame on index:\\n\", joined_outer_df)\n",
    "\n",
    "# Joining on a column (similar to merge)\n",
    "df_join_left_col = pd.DataFrame({\n",
    "    'key': ['K0', 'K1', 'K2'],\n",
    "    'A': ['A0', 'A1', 'A2']\n",
    "})\n",
    "df_join_right_col_idx = pd.DataFrame({'B': ['B0', 'B1', 'B2']}, index=['K0', 'K1', 'K2'])\n",
    "print(\"\\ndf_join_left_col:\\n\", df_join_left_col)\n",
    "print(\"df_join_right_col_idx (index matches 'key' in left):\\n\", df_join_right_col_idx)\n",
    "joined_on_col_to_idx = df_join_left_col.join(df_join_right_col_idx, on='key') # Joins df_join_left_col's 'key' column with df_join_right_col_idx's index\n",
    "print(\"\\nJoined on df_join_left_col's 'key' column to df_join_right_col_idx's index:\\n\", joined_on_col_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 10. Time Series Data ---\n",
    "# Pandas has excellent support for time series data.\n",
    "\n",
    "print(\"\\n\\n--- 10. Time Series Data ---\")\n",
    "# Create a date range\n",
    "date_rng = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D') # 'D' for daily frequency\n",
    "print(\"Date range:\\n\", date_rng)\n",
    "\n",
    "# Create a time series DataFrame\n",
    "ts_df = pd.DataFrame(date_rng, columns=['date'])\n",
    "ts_df['data'] = np.random.randint(0, 100, size=(len(date_rng))) # Add some random data\n",
    "ts_df = ts_df.set_index('date') # Set the 'date' column as the index\n",
    "print(\"\\nTime series DataFrame (ts_df):\\n\", ts_df)\n",
    "\n",
    "# Selecting data for a specific date\n",
    "print(\"\\nData for 2023-01-03:\\n\", ts_df.loc['2023-01-03'])\n",
    "\n",
    "# Selecting data for a date range\n",
    "print(\"\\nData from 2023-01-02 to 2023-01-05:\\n\", ts_df['2023-01-02':'2023-01-05'])\n",
    "\n",
    "# Resampling time series data\n",
    "# Resample to 3-day frequency and calculate mean\n",
    "resampled_mean = ts_df['data'].resample('3D').mean() # '3D' for 3-day frequency\n",
    "print(\"\\nData resampled to 3-day mean:\\n\", resampled_mean)\n",
    "\n",
    "# Resample to business day frequency and sum\n",
    "resampled_sum_bizday = ts_df['data'].resample('B').sum() # 'B' for business day frequency\n",
    "print(\"\\nData resampled to business day sum:\\n\", resampled_sum_bizday)\n",
    "\n",
    "# Shifting and lagging data (useful for time series analysis)\n",
    "ts_df['data_lagged_1'] = ts_df['data'].shift(1) # Shifts data down by 1 period\n",
    "ts_df['data_lead_1'] = ts_df['data'].shift(-1) # Shifts data up by 1 period\n",
    "print(\"\\nTime series DataFrame with lagged and lead data:\\n\", ts_df)\n",
    "\n",
    "# Rolling window calculations\n",
    "ts_df['rolling_mean_3D'] = ts_df['data'].rolling(window=3).mean() # Calculates 3-period rolling mean\n",
    "print(\"\\nTime series DataFrame with 3-day rolling mean:\\n\", ts_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 11. Basic Plotting ---\n",
    "# Pandas integrates with Matplotlib for basic plotting.\n",
    "# For more advanced plots, libraries like Seaborn or Plotly are often used.\n",
    "# You might need to install matplotlib: pip install matplotlib\n",
    "\n",
    "print(\"\\n\\n--- 11. Basic Plotting ---\")\n",
    "# Ensure matplotlib is available for plotting in some environments (like scripts)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt # Commonly aliased as plt\n",
    "    # Simple line plot of the 'data' column from ts_df\n",
    "    ts_df['data'].plot(kind='line', title='Time Series Data Plot')\n",
    "    plt.ylabel('Value') # Set y-axis label\n",
    "    plt.show() # Display the plot\n",
    "\n",
    "    # Bar plot of mean salary per department\n",
    "    mean_salary_dept.plot(kind='bar', title='Mean Salary by Department')\n",
    "    plt.ylabel('Mean Salary') # Set y-axis label\n",
    "    plt.xlabel('Department') # Set x-axis label\n",
    "    plt.xticks(rotation=45) # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout() # Adjust plot to prevent labels from overlapping\n",
    "    plt.show() # Display the plot\n",
    "\n",
    "    # Histogram of 'Age' from df1\n",
    "    df1['Age'].plot(kind='hist', bins=5, title='Age Distribution')\n",
    "    plt.xlabel('Age') # Set x-axis label\n",
    "    plt.ylabel('Frequency') # Set y-axis label\n",
    "    plt.show() # Display the plot\n",
    "\n",
    "    print(\"Plotting examples executed. Check for plot windows if running locally.\")\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not installed. Skipping plotting examples. Install with: pip install matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 12. Advanced Topics (Brief Mention) ---\n",
    "print(\"\\n\\n--- 12. Advanced Topics (Brief Mention) ---\")\n",
    "\n",
    "# --- MultiIndex ---\n",
    "# Allows you to have multiple levels of indexing on rows and/or columns.\n",
    "arrays = [\n",
    "    ['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n",
    "    ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']\n",
    "]\n",
    "multi_idx = pd.MultiIndex.from_arrays(arrays, names=('first', 'second')) # Creates a MultiIndex\n",
    "s_multi = pd.Series(np.random.randn(8), index=multi_idx)\n",
    "print(\"Series with MultiIndex:\\n\", s_multi)\n",
    "print(\"\\nAccessing 'bar' level from MultiIndex Series:\\n\", s_multi['bar'])\n",
    "print(\"\\nAccessing ('bar', 'one') from MultiIndex Series:\", s_multi['bar', 'one'])\n",
    "\n",
    "df_multi = pd.DataFrame(np.random.randn(8, 2), index=multi_idx, columns=['A', 'B'])\n",
    "print(\"\\nDataFrame with MultiIndex:\\n\", df_multi)\n",
    "print(\"\\nAccessing 'baz' level from MultiIndex DataFrame:\\n\", df_multi.loc['baz'])\n",
    "\n",
    "# --- Working with Large Datasets (Chunking) ---\n",
    "# When dealing with very large files that don't fit into memory,\n",
    "# you can read and process them in chunks.\n",
    "# Example:\n",
    "# chunk_iter = pd.read_csv('very_large_file.csv', chunksize=10000) # Creates an iterator\n",
    "# for chunk in chunk_iter:\n",
    "#     # Process each chunk (e.g., perform calculations, filter)\n",
    "#     # processed_chunk = chunk[chunk['some_column'] > value]\n",
    "#     # print(processed_chunk.head())\n",
    "#     pass # Placeholder for actual processing\n",
    "print(\"\\nChunking example (conceptual): pd.read_csv('large_file.csv', chunksize=10000)\")\n",
    "\n",
    "# --- Performance Optimization ---\n",
    "# Pandas can be very fast, but for extremely large datasets or complex operations:\n",
    "# - Use vectorized operations (like s1 + 5) instead of loops where possible.\n",
    "# - Choose appropriate data types (e.g., 'category' for low-cardinality string columns).\n",
    "# - Be mindful of copying data vs. using views.\n",
    "# - For very demanding tasks, consider libraries like Dask, Vaex, or Modin which can parallelize Pandas operations.\n",
    "print(\"\\nPerformance tips: Use vectorized operations, appropriate dtypes, consider Dask/Modin for very large data.\")\n",
    "\n",
    "print(\"\\n\\n--- Pandas Tutorial Complete! ---\")\n",
    "print(\"This tutorial covered the basics to some advanced features of Pandas.\")\n",
    "print(\"Keep practicing and exploring the extensive Pandas documentation for more!\")\n",
    "\n",
    "# Clean up the sample CSV file created earlier\n",
    "import os\n",
    "if os.path.exists('sample_data.csv'):\n",
    "    os.remove('sample_data.csv')\n",
    "    print(\"\\nCleaned up 'sample_data.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
